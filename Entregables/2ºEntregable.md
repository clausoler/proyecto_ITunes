
# 2. ETL y Preprocesamiento

## ğŸŸ  ExtracciÃ³n

La fuente principal de datos del proyecto es la **API pÃºblica de iTunes**, la cual permite acceder a informaciÃ³n detallada sobre canciones, Ã¡lbumes y artistas. Sin embargo, esta API **no dispone de un endpoint que devuelva listados completos** de canciones o Ã¡lbumes, por lo que se ha tenido que diseÃ±ar una estrategia alternativa para maximizar la cantidad de datos extraÃ­dos.

Para realizar la extracciÃ³n:

- Se ha implementado una tÃ©cnica de **combinaciones de tÃ©rminos**: se generan combinaciones de dos letras del alfabeto (de `aa` a `zz`) para usarlas como tÃ©rminos de bÃºsqueda.
- Cada dÃ­a se consultan **97 tÃ©rminos distintos**, lo que permite descargar hasta **19.400 canciones por dÃ­a**, dada la limitaciÃ³n de 200 resultados por tÃ©rmino impuesta por la API.
- Este proceso se ha ejecutado durante **7 dÃ­as consecutivos**, para ampliar la cobertura de datos.

El acceso a la API se realiza mediante un archivo `.env` que contiene la URL de la API, la cual se carga dinÃ¡micamente en el cÃ³digo mediante `os.getenv()`, asegurando que las credenciales y configuraciones se mantengan seguras y fuera del cÃ³digo fuente.

Los datos extraÃ­dos incluyen, entre otros:

- TÃ­tulo de la canciÃ³n
- Nombre del Ã¡lbum
- GÃ©nero musical
- DuraciÃ³n (en milisegundos)
- Precio de la canciÃ³n
- Precio del Ã¡lbum
- Fecha de publicaciÃ³n

---

## ğŸŸ¡ TransformaciÃ³n

Una vez extraÃ­dos los datos desde la API de iTunes, se lleva a cabo un proceso de limpieza y transformaciÃ³n exhaustivo para garantizar su calidad y consistencia antes de ser cargados a la base de datos.

### ğŸ§¹ Limpieza de datos

Se han aplicado mÃºltiples tÃ©cnicas de depuraciÃ³n y estandarizaciÃ³n:

- Se utiliza la librerÃ­a **`unicodedata`** para limpiar todas las columnas de tipo texto (`object`) del `DataFrame`. Esta limpieza incluye:
  - EliminaciÃ³n de tildes y conversiÃ³n de caracteres Unicode a ASCII.
  - SupresiÃ³n de espacios extra y caracteres especiales mediante expresiones regulares.
  - EliminaciÃ³n de filas en las que todas las columnas de texto quedaron vacÃ­as tras el proceso.
- Las columnas de tipo **fecha** son procesadas para separar las partes de hora, minuto y segundo, convirtiÃ©ndolas luego al formato `datetime64[ns]`.
- Se realiza conversiÃ³n de tipos:
  - De `float` a `int` para identificadores o cantidades.
  - De `str` a `bool` para columnas que representan valores lÃ³gicos.
- En cuanto a valores nulos:
  - Se eliminan registros con nulos en columnas que contienen identificadores Ãºnicos.
  - En columnas de texto, los nulos se reemplazan por `"Sin identificar"`.
  - En columnas de precios, los valores negativos se consideran errÃ³neos, se reemplazan por `NaN` y luego se imputan usando la **media real** de los precios vÃ¡lidos.
- Para evitar duplicados:
  - Se carga un `DataFrame` maestro desde un archivo `.pkl`.
  - Se divide y normaliza la informaciÃ³n en varias tablas: `Artist`, `Album`, `Track`, `Genre`, `Track_prices`, `Album_prices`.
  - Se garantiza la unicidad de claves primarias y se eliminan duplicados histÃ³ricos (por ID + fecha).

### ğŸ”§ Transformaciones aplicadas

- Se estandarizan los nombres de columnas para que coincidan exactamente con los nombres utilizados en la base de datos SQL destino.
- No se han realizado agregaciones estadÃ­sticas, ya que cada fila contiene informaciÃ³n Ãºnica y no repetida.

### ğŸ§° LibrerÃ­as utilizadas

AdemÃ¡s de **`pandas`**, se han utilizado las siguientes herramientas y librerÃ­as:

- `numpy`
- `re`
- `unicodedata`
- `glob`
- `matplotlib.pyplot`
- `seaborn`

---

## ğŸŸ¢ Carga

Una vez transformados y depurados, los datos se cargan en dos formatos distintos:

1. **Archivos locales:**  
   Se guardan como archivos `.pkl` individuales, uno por cada tabla del modelo de datos. Esto permite un acceso rÃ¡pido para anÃ¡lisis exploratorios o pruebas sin necesidad de consultar la base de datos. EstÃ¡n omitidos en el archivo .gitignore porque exceden el lÃ­mite de GB en GitHub. 

2. **Base de datos relacional PostgreSQL:**  
   Los datos limpios se cargan tambiÃ©n en una base de datos PostgreSQL, estructurada en un esquema relacional que facilita su consulta y anÃ¡lisis posterior.

### ğŸ—ƒï¸ Estructura de la base de datos

Los datos estÃ¡n organizados en mÃºltiples tablas normalizadas que siguen una arquitectura de base de datos clÃ¡sica. Entre ellas:

- `artist`
- `album`
- `track`
- `genre`
- `track_prices`
- `album_prices`

Cada tabla contiene claves primarias y, en algunos casos, claves forÃ¡neas que establecen relaciones entre entidades (por ejemplo, entre `track` y `album`, o entre `album` y `artist`).

### ğŸ”Œ Carga con `psycopg2`

Para cargar los datos en PostgreSQL se ha utilizado la librerÃ­a `psycopg2`, que permite ejecutar sentencias SQL directamente desde Python. La conexiÃ³n se configura de forma segura a travÃ©s de variables de entorno, evitando exponer credenciales en el cÃ³digo.

Este proceso incluye:

- Inserciones seguras con control de duplicados.
- ValidaciÃ³n de integridad referencial mediante claves forÃ¡neas.
- ConversiÃ³n automÃ¡tica de tipos (`datetime`, `int`, `float`, `str`) en la inserciÃ³n.

---

## ğŸ”µ DocumentaciÃ³n tÃ©cnica y reproducibilidad

El proyecto ha sido diseÃ±ado para ser modular, reproducible y mantenible. Aunque actualmente el pipeline se ejecuta desde un conjunto de notebooks, estÃ¡ planificado migrar su ejecuciÃ³n hacia un script principal `main.py` que orqueste todo el flujo desde `src/`.

### ğŸ—‚ï¸ Estructura del proyecto

El proyecto sigue una organizaciÃ³n clara de carpetas:

```
project-root/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_raw/       # Datos originales extraÃ­dos desde la API
â”‚   â””â”€â”€ data_clean/     # Datos transformados y listos para el anÃ¡lisis
â”œâ”€â”€ notebooks/          # Notebooks para exploraciÃ³n y pruebas (extracciÃ³n, limpieza, carga)
â”œâ”€â”€ src/                # Scripts Python con funciones modulares
â”‚   â”œâ”€â”€ extraccion.py
â”‚   â”œâ”€â”€ limpieza.py
â”‚   â””â”€â”€ carga.py
â”œâ”€â”€ .env                # Variables de entorno (API URL, credenciales)
â””â”€â”€ entregables/        # Archivos para entrega del proyecto final
```

### ğŸ” Reproducibilidad

Para garantizar la reproducibilidad del pipeline:

- Se utiliza un archivo `.env` para manejar variables sensibles (como la URL de la API o parÃ¡metros de conexiÃ³n a la base de datos).
- Las funciones estÃ¡n modularizadas y separadas por archivo en la carpeta `src/`, lo que facilita su reutilizaciÃ³n.
- Se registra el avance del scraping mediante un archivo de log de tÃ©rminos ya utilizados (`terminos_usados.txt`), evitando redundancias.
- EstÃ¡ previsto incluir un `requirements.txt` con todas las dependencias del entorno, asegurando que el entorno pueda recrearse fÃ¡cilmente en otras mÃ¡quinas o entornos virtuales.

### â±ï¸ Tiempo de ejecuciÃ³n

Cada notebook (o script equivalente) tarda menos de **1 minuto** en ejecutarse, gracias a una lÃ³gica optimizada y uso de estructuras eficientes como `DataFrames`, acceso por lotes, y tratamiento vectorizado.
